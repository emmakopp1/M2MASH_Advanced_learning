{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica-Bold;
\f3\froman\fcharset0 Times-Bold;\f4\fnil\fcharset0 AppleSymbols;\f5\ftech\fcharset77 Symbol;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;\red0\green0\blue0;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c93333;\csgray\c0\c0;\cssrgb\c0\c0\c0;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh18000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Link of the book with the figures (Francis Bach) : 
\f1\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\ul \ulc2 \outl0\strokewidth0 \strokec2 https://www.di.ens.fr/~fbach/ltfp_book.pdf
\f0\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \ulnone \outl0\strokewidth0 \
\
\

\f2\b Figure 3.1 :  
\f3\fs28 \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Liner Least-squares regression\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Setting : Ordinary Least Square\

\f3\b \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 Polynomial regression with varying number of observations.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f3\b\fs28 \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 \

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 Figure 3.2 :  
\f3\fs28 \cf4 \cb3 \expnd0\expndtw0\kerning0
Liner Least-squares regression\
\pard\pardeftab720\partightenfactor0

\f1\b0 \cf4 Setting : Ordinary Least Square\

\f3\b \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b0 \cf4 Convergence rate for polynomial regression with error bars (obtained from 32 replications by adding/subtracting standard deviations), plotted in logarithmic scale, with fixed design (left plot). The large error bars for small n in the right plot are due to the lower error bar being negative before taking the logarithm. 
\f3\b \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\
\pard\pardeftab720\partightenfactor0

\f3\b\fs28 \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Figure p.90 : \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Optimization for machine learning\

\f1\b0 Setting : Ordinary Least Square\

\f3\b \

\f1\b0 We consider two quadratic optimization problems in dimension d = 1000, with two different decays of eigenvalues (lambda_k) for the Hessian matrix H, one as 1/k (in blue below) and one in 1/k^2 (in red below), and for which we plot the performance for function values, both in semi-logarithm plots (left) and full-logarithm plots (right).\
For slow decays (blue), we see the linear convergence kicking in, while for fast decays (red), the rates in 1/t dominate.
\f3\b \
\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
Figure p108  : Optimization for machine learning
\f1\b0 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Setting : SVM.\cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf4 \
We consider a simple binary classification problem with linear predictors\
and features with l2-norm bounded by R. We consider the hinge loss with a square l2-regularizer \uc0\u956  
\f4 \uc0\u8741 
\f1  \'b7 
\f4 \uc0\u8741 
\f1 2. \
We measure the excess training objective. \
We consider two values of \uc0\u956 , and compare the two step-sizes \u947 t = 1/(R2 t) and \u947 t = 1/(\u956 t). \
We see that for large enough \uc0\u956 , the strongly-convex step-size is better. This is not the case for small \u956 .\
\
\pard\pardeftab720\partightenfactor0

\f3\b \cf4 Figure 6.2 : Local Averaging Methods
\f1\b0 \
Setting : Partition estimator\
\
Regressograms in d = 1 dimension, with three values of |J| (the number of sets in the partition). \
We can observe both underfitting (|J| too small), or overfitting (|J| too large). \
Note that the target function f
\f5 \uc0\u8727 
\f1  is piecewise affine, and that on the affine parts, the estimator is far from linear, that is, the estimator cannot take advantage of extra-regularity. \
\
\pard\pardeftab720\partightenfactor0

\f3\b \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Figure 6.3 : Local Averaging Methods
\f1\b0 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
\pard\pardeftab720\partightenfactor0
\cf4 Setting : Nearest neighbors.\
\
k-nearest neighbor regression in $d = 1$ dimension, with 3 values of k (the number of neighbors). We can observe both underfitting (k too large), or overfitting (k too small).\
\
\pard\pardeftab720\partightenfactor0

\f3\b \cf4 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Figure 6.4 : Local Averaging Methods
\f1\b0 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf4 Nadaraya-Watson\
\
Nadaraya-Watson regression in $d = 1$ dimension, with three values of h (the bandwidth), for the Gaussian kernel. \
We can observe both underfitting (h too large), or overfitting (h too small).}